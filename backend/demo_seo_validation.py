#!/usr/bin/env python3
"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π SEO –≤–∞–ª–∏–¥–∞—Ç–æ—Ä
==============================

–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ –∫—Ä–æ—Å—Å-—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è SEO.
"""

import asyncio
import json
import requests
import time
from datetime import datetime
import re
from urllib.parse import urlparse
from typing import Dict, List, Any

class SimpleSEOValidator:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π SEO –≤–∞–ª–∏–¥–∞—Ç–æ—Ä –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    
    def __init__(self):
        self.timeout = 10
    
    def validate_url(self, url: str) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è URL —Å –±–∞–∑–æ–≤—ã–º–∏ SEO –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏"""
        
        print(f"üîç –ê–Ω–∞–ª–∏–∑ URL: {url}")
        start_time = time.time()
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            response = requests.get(url, timeout=self.timeout)
            response.raise_for_status()
            html_content = response.text
            load_time = time.time() - start_time
            
            # –ü—Ä–æ–≤–æ–¥–∏–º –∞–Ω–∞–ª–∏–∑
            results = {
                'url': url,
                'timestamp': datetime.now().isoformat(),
                'load_time': round(load_time, 3),
                'status_code': response.status_code,
                'content_length': len(html_content),
                'analysis': {}
            }
            
            # –ê–Ω–∞–ª–∏–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö SEO —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            results['analysis'] = {
                'title': self._analyze_title(html_content),
                'meta_description': self._analyze_meta_description(html_content),
                'headings': self._analyze_headings(html_content),
                'images': self._analyze_images(html_content),
                'links': self._analyze_links(html_content, url),
                'structured_data': self._analyze_structured_data(html_content),
                'technical': self._analyze_technical_aspects(url, response.headers),
                'content_quality': self._analyze_content_quality(html_content)
            }
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É
            results['seo_score'] = self._calculate_seo_score(results['analysis'])
            results['recommendations'] = self._generate_recommendations(results['analysis'])
            results['issues'] = self._extract_issues(results['analysis'])
            
            print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {load_time:.2f} —Å–µ–∫")
            return results
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")
            return {
                'url': url,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def _analyze_title(self, html_content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ title —Ç–µ–≥–∞"""
        
        title_match = re.search(r'<title[^>]*>(.*?)</title>', html_content, re.IGNORECASE | re.DOTALL)
        
        if not title_match:
            return {
                'exists': False,
                'content': '',
                'length': 0,
                'issues': ['–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç title —Ç–µ–≥'],
                'score': 0
            }
        
        title = title_match.group(1).strip()
        title_length = len(title)
        
        issues = []
        score = 100
        
        if title_length < 30:
            issues.append('Title —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (–º–µ–Ω–µ–µ 30 —Å–∏–º–≤–æ–ª–æ–≤)')
            score -= 30
        elif title_length > 60:
            issues.append('Title —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (–±–æ–ª–µ–µ 60 —Å–∏–º–≤–æ–ª–æ–≤)')
            score -= 20
        
        if not title:
            issues.append('–ü—É—Å—Ç–æ–π title —Ç–µ–≥')
            score = 0
        
        return {
            'exists': True,
            'content': title,
            'length': title_length,
            'issues': issues,
            'score': max(0, score)
        }
    
    def _analyze_meta_description(self, html_content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ meta description"""
        
        meta_desc_pattern = r'<meta[^>]*name=["\']description["\'][^>]*content=["\']([^"\']*)["\']'
        match = re.search(meta_desc_pattern, html_content, re.IGNORECASE)
        
        if not match:
            return {
                'exists': False,
                'content': '',
                'length': 0,
                'issues': ['–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç meta description'],
                'score': 0
            }
        
        description = match.group(1).strip()
        desc_length = len(description)
        
        issues = []
        score = 100
        
        if desc_length < 120:
            issues.append('Meta description —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (–º–µ–Ω–µ–µ 120 —Å–∏–º–≤–æ–ª–æ–≤)')
            score -= 30
        elif desc_length > 160:
            issues.append('Meta description —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (–±–æ–ª–µ–µ 160 —Å–∏–º–≤–æ–ª–æ–≤)')
            score -= 20
        
        return {
            'exists': True,
            'content': description,
            'length': desc_length,
            'issues': issues,
            'score': max(0, score)
        }
    
    def _analyze_headings(self, html_content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
        
        headings = {}
        issues = []
        score = 100
        
        for i in range(1, 7):
            pattern = f'<h{i}[^>]*>(.*?)</h{i}>'
            matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
            headings[f'h{i}'] = {
                'count': len(matches),
                'content': [match.strip() for match in matches]
            }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º H1
        if headings['h1']['count'] == 0:
            issues.append('–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç H1 –∑–∞–≥–æ–ª–æ–≤–æ–∫')
            score -= 40
        elif headings['h1']['count'] > 1:
            issues.append('–ù–µ—Å–∫–æ–ª—å–∫–æ H1 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ')
            score -= 20
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        if headings['h2']['count'] == 0:
            issues.append('–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç H2 –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞')
            score -= 15
        
        return {
            'structure': headings,
            'issues': issues,
            'score': max(0, score)
        }
    
    def _analyze_images(self, html_content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        
        img_tags = re.findall(r'<img[^>]*>', html_content, re.IGNORECASE)
        
        total_images = len(img_tags)
        images_with_alt = 0
        
        for img in img_tags:
            if 'alt=' in img.lower():
                images_with_alt += 1
        
        images_without_alt = total_images - images_with_alt
        
        issues = []
        score = 100
        
        if images_without_alt > 0:
            issues.append(f'{images_without_alt} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ alt –∞—Ç—Ä–∏–±—É—Ç–∞')
            score -= min(50, images_without_alt * 10)
        
        return {
            'total_images': total_images,
            'images_with_alt': images_with_alt,
            'images_without_alt': images_without_alt,
            'issues': issues,
            'score': max(0, score)
        }
    
    def _analyze_links(self, html_content: str, base_url: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫"""
        
        link_pattern = r'<a[^>]*href=["\']([^"\']*)["\'][^>]*>(.*?)</a>'
        links = re.findall(link_pattern, html_content, re.IGNORECASE | re.DOTALL)
        
        total_links = len(links)
        internal_links = 0
        external_links = 0
        empty_anchor_links = 0
        
        base_domain = urlparse(base_url).netloc
        
        for href, anchor_text in links:
            # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Å—Å—ã–ª–∫–∏
            if href.startswith('http'):
                link_domain = urlparse(href).netloc
                if link_domain == base_domain:
                    internal_links += 1
                else:
                    external_links += 1
            else:
                internal_links += 1
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º anchor text
            if not anchor_text.strip():
                empty_anchor_links += 1
        
        issues = []
        score = 100
        
        if empty_anchor_links > 0:
            issues.append(f'{empty_anchor_links} —Å—Å—ã–ª–æ–∫ —Å –ø—É—Å—Ç—ã–º anchor text')
            score -= min(30, empty_anchor_links * 5)
        
        return {
            'total_links': total_links,
            'internal_links': internal_links,
            'external_links': external_links,
            'empty_anchor_links': empty_anchor_links,
            'issues': issues,
            'score': max(0, score)
        }
    
    def _analyze_structured_data(self, html_content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        # JSON-LD
        json_ld_pattern = r'<script type="application/ld\+json"[^>]*>(.*?)</script>'
        json_ld_matches = re.findall(json_ld_pattern, html_content, re.IGNORECASE | re.DOTALL)
        
        valid_schemas = 0
        errors = []
        
        for match in json_ld_matches:
            try:
                data = json.loads(match.strip())
                if '@type' in data:
                    valid_schemas += 1
            except json.JSONDecodeError:
                errors.append('–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON-LD')
        
        # –ú–∏–∫—Ä–æ–¥–∞–Ω–Ω—ã–µ
        microdata_pattern = r'itemscope[^>]*itemtype="([^"]*)"'
        microdata_matches = re.findall(microdata_pattern, html_content, re.IGNORECASE)
        
        issues = []
        score = 50  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        
        if valid_schemas > 0:
            score += min(30, valid_schemas * 15)
        else:
            issues.append('–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç JSON-LD —Ä–∞–∑–º–µ—Ç–∫–∞')
        
        if microdata_matches:
            score += 10
        
        if errors:
            score -= len(errors) * 10
            issues.extend(errors)
        
        return {
            'json_ld_count': len(json_ld_matches),
            'valid_schemas': valid_schemas,
            'microdata_count': len(microdata_matches),
            'errors': errors,
            'issues': issues,
            'score': max(0, min(100, score))
        }
    
    def _analyze_technical_aspects(self, url: str, headers: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∞—Å–ø–µ–∫—Ç–æ–≤"""
        
        issues = []
        score = 100
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º HTTPS
        is_https = url.startswith('https://')
        if not is_https:
            issues.append('–°–∞–π—Ç –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç HTTPS')
            score -= 30
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        content_type = headers.get('content-type', '').lower()
        if 'charset=utf-8' not in content_type:
            issues.append('–ù–µ —É–∫–∞–∑–∞–Ω–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∞ UTF-8')
            score -= 10
        
        return {
            'is_https': is_https,
            'content_type': content_type,
            'headers': dict(headers),
            'issues': issues,
            'score': max(0, score)
        }
    
    def _analyze_content_quality(self, html_content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
        text_content = re.sub(r'<[^>]+>', ' ', html_content)
        text_content = re.sub(r'\s+', ' ', text_content).strip()
        
        words = text_content.split()
        sentences = text_content.split('.')
        
        word_count = len(words)
        sentence_count = len(sentences)
        
        issues = []
        score = 100
        
        if word_count < 300:
            issues.append('–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–º–µ–Ω–µ–µ 300 —Å–ª–æ–≤)')
            score -= 40
        
        if sentence_count > 0:
            avg_sentence_length = word_count / sentence_count
            if avg_sentence_length > 25:
                issues.append('–°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—É—Ö—É–¥—à–∞–µ—Ç —á–∏—Ç–∞–µ–º–æ—Å—Ç—å)')
                score -= 20
        
        return {
            'word_count': word_count,
            'sentence_count': sentence_count,
            'avg_sentence_length': round(word_count / sentence_count, 1) if sentence_count > 0 else 0,
            'issues': issues,
            'score': max(0, score)
        }
    
    def _calculate_seo_score(self, analysis: Dict[str, Any]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â—É—é SEO –æ—Ü–µ–Ω–∫—É"""
        
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        weights = {
            'title': 0.2,
            'meta_description': 0.15,
            'headings': 0.15,
            'images': 0.1,
            'links': 0.1,
            'structured_data': 0.1,
            'technical': 0.1,
            'content_quality': 0.1
        }
        
        total_score = 0
        total_weight = 0
        
        for category, weight in weights.items():
            if category in analysis and 'score' in analysis[category]:
                total_score += analysis[category]['score'] * weight
                total_weight += weight
        
        return round(total_score / total_weight if total_weight > 0 else 0, 2)
    
    def _generate_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
        
        recommendations = []
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ title
        if analysis['title']['score'] < 70:
            recommendations.append('–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ title —Ç–µ–≥ (30-60 —Å–∏–º–≤–æ–ª–æ–≤)')
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ meta description
        if analysis['meta_description']['score'] < 70:
            recommendations.append('–î–æ–±–∞–≤—å—Ç–µ –∏–ª–∏ —É–ª—É—á—à–∏—Ç–µ meta description (120-160 —Å–∏–º–≤–æ–ª–æ–≤)')
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
        if analysis['headings']['score'] < 70:
            recommendations.append('–£–ª—É—á—à–∏—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ H1, –¥–æ–±–∞–≤—å—Ç–µ H2-H3)')
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
        if analysis['images']['score'] < 80:
            recommendations.append('–î–æ–±–∞–≤—å—Ç–µ alt –∞—Ç—Ä–∏–±—É—Ç—ã –∫–æ –≤—Å–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º')
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
        if analysis['structured_data']['score'] < 50:
            recommendations.append('–î–æ–±–∞–≤—å—Ç–µ JSON-LD —Ä–∞–∑–º–µ—Ç–∫—É Schema.org')
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –∞—Å–ø–µ–∫—Ç–∞–º
        if analysis['technical']['score'] < 80:
            recommendations.append('–ò—Å–ø—Ä–∞–≤—å—Ç–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã (HTTPS, –∫–æ–¥–∏—Ä–æ–≤–∫–∞)')
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É
        if analysis['content_quality']['score'] < 70:
            recommendations.append('–£–≤–µ–ª–∏—á—å—Ç–µ –æ–±—ä–µ–º –∏ —É–ª—É—á—à–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞')
        
        return recommendations
    
    def _extract_issues(self, analysis: Dict[str, Any]) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã"""
        
        all_issues = []
        
        for category, data in analysis.items():
            if 'issues' in data:
                all_issues.extend(data['issues'])
        
        return all_issues

def format_results(results: Dict[str, Any]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤—ã–≤–æ–¥–∞"""
    
    if 'error' in results:
        return f"‚ùå –û—à–∏–±–∫–∞: {results['error']}"
    
    output = []
    output.append(f"üìä SEO –ê–Ω–∞–ª–∏–∑: {results['url']}")
    output.append(f"‚è±Ô∏è  –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {results['load_time']} —Å–µ–∫")
    output.append(f"üìà SEO –æ—Ü–µ–Ω–∫–∞: {results['seo_score']}/100")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    analysis = results['analysis']
    
    output.append(f"\nüìù –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑:")
    output.append(f"  Title: {analysis['title']['score']}/100 ({analysis['title']['length']} —Å–∏–º–≤–æ–ª–æ–≤)")
    output.append(f"  Meta Description: {analysis['meta_description']['score']}/100 ({analysis['meta_description']['length']} —Å–∏–º–≤–æ–ª–æ–≤)")
    output.append(f"  –ó–∞–≥–æ–ª–æ–≤–∫–∏: {analysis['headings']['score']}/100")
    output.append(f"  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {analysis['images']['score']}/100 ({analysis['images']['images_without_alt']} –±–µ–∑ alt)")
    output.append(f"  –°—Å—ã–ª–∫–∏: {analysis['links']['score']}/100 ({analysis['links']['total_links']} –≤—Å–µ–≥–æ)")
    output.append(f"  –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {analysis['structured_data']['score']}/100")
    output.append(f"  –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã: {analysis['technical']['score']}/100")
    output.append(f"  –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {analysis['content_quality']['score']}/100 ({analysis['content_quality']['word_count']} —Å–ª–æ–≤)")
    
    # –ü—Ä–æ–±–ª–µ–º—ã
    if results['issues']:
        output.append(f"\n‚ùå –ù–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:")
        for issue in results['issues'][:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            output.append(f"  ‚Ä¢ {issue}")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if results['recommendations']:
        output.append(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        for rec in results['recommendations'][:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            output.append(f"  + {rec}")
    
    return '\n'.join(output)

def save_results(results: Dict[str, Any], filename: str):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON —Ñ–∞–π–ª"""
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {filename}")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ URL
    test_urls = [
        "http://localhost:8000/good_seo_test.html",
        "http://localhost:8000/bad_seo_test.html",
        "http://localhost:8000/ecommerce_test.html"
    ]
    
    validator = SimpleSEOValidator()
    all_results = []
    
    print("üöÄ –ó–∞–ø—É—Å–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ SEO —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    print("=" * 60)
    
    for i, url in enumerate(test_urls, 1):
        print(f"\n[{i}/{len(test_urls)}] –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: {url}")
        print("-" * 50)
        
        # –ü—Ä–æ–≤–æ–¥–∏–º –∞–Ω–∞–ª–∏–∑
        results = validator.validate_url(url)
        all_results.append(results)
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print(format_results(results))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if 'error' not in results:
            filename = f"seo_demo_{url.split('/')[-1].replace('.html', '')}.json"
            save_results(results, filename)
    
    # –°–≤–æ–¥–∫–∞ –ø–æ –≤—Å–µ–º —Ç–µ—Å—Ç–∞–º
    print(f"\n{'='*60}")
    print("üìã –°–í–û–î–ö–ê –ü–û –í–°–ï–ú –¢–ï–°–¢–ê–ú")
    print("=" * 60)
    
    successful_tests = [r for r in all_results if 'error' not in r]
    failed_tests = [r for r in all_results if 'error' in r]
    
    print(f"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {len(successful_tests)}")
    print(f"‚ùå –ù–µ—É–¥–∞—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {len(failed_tests)}")
    
    if successful_tests:
        avg_score = sum(r['seo_score'] for r in successful_tests) / len(successful_tests)
        best_url = max(successful_tests, key=lambda x: x['seo_score'])
        worst_url = min(successful_tests, key=lambda x: x['seo_score'])
        
        print(f"üìà –°—Ä–µ–¥–Ω—è—è SEO –æ—Ü–µ–Ω–∫–∞: {avg_score:.1f}/100")
        print(f"üèÜ –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best_url['url']} ({best_url['seo_score']}/100)")
        print(f"üî¥ –¢—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è: {worst_url['url']} ({worst_url['seo_score']}/100)")
        
        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        all_issues = []
        for result in successful_tests:
            all_issues.extend(result['issues'])
        
        if all_issues:
            from collections import Counter
            common_issues = Counter(all_issues).most_common(3)
            
            print(f"\nüéØ –ù–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:")
            for issue, count in common_issues:
                print(f"  ‚Ä¢ {issue} (–≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è {count} —Ä–∞–∑)")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â–∏–π –æ—Ç—á–µ—Ç
    full_report = {
        'timestamp': datetime.now().isoformat(),
        'summary': {
            'total_tests': len(all_results),
            'successful': len(successful_tests),
            'failed': len(failed_tests),
            'average_score': avg_score if successful_tests else 0
        },
        'results': all_results
    }
    
    save_results(full_report, 'seo_demo_full_report.json')
    
    print(f"\nüéâ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

if __name__ == "__main__":
    main()
