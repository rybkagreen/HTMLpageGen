#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π SEO –≤–∞–ª–∏–¥–∞—Ç–æ—Ä
========================

–†–∞–±–æ—Ç–∞–µ—Ç —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ HTML —Ñ–∞–π–ª–∞–º–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ –∫—Ä–æ—Å—Å-—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è SEO.
"""

import json
import time
from datetime import datetime
import re
from pathlib import Path
from typing import Dict, List, Any

class OfflineSEOValidator:
    """–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π SEO –≤–∞–ª–∏–¥–∞—Ç–æ—Ä –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    
    def __init__(self):
        self.timestamp = datetime.now()
    
    def validate_file(self, file_path: str) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è HTML —Ñ–∞–π–ª–∞ —Å –±–∞–∑–æ–≤—ã–º–∏ SEO –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏"""
        
        print(f"üîç –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {file_path}")
        start_time = time.time()
        
        try:
            # –ß–∏—Ç–∞–µ–º HTML —Ñ–∞–π–ª
            with open(file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            load_time = time.time() - start_time
            
            # –ü—Ä–æ–≤–æ–¥–∏–º –∞–Ω–∞–ª–∏–∑
            results = {
                'file_path': file_path,
                'timestamp': datetime.now().isoformat(),
                'load_time': round(load_time, 3),
                'content_length': len(html_content),
                'analysis': {}
            }
            
            # –ê–Ω–∞–ª–∏–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö SEO —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            results['analysis'] = {
                'title': self._analyze_title(html_content),
                'meta_description': self._analyze_meta_description(html_content),
                'meta_viewport': self._analyze_meta_viewport(html_content),
                'headings': self._analyze_headings(html_content),
                'images': self._analyze_images(html_content),
                'links': self._analyze_links(html_content),
                'structured_data': self._analyze_structured_data(html_content),
                'technical': self._analyze_technical_aspects(html_content),
                'content_quality': self._analyze_content_quality(html_content),
                'open_graph': self._analyze_open_graph(html_content),
                'lang_attribute': self._analyze_lang_attribute(html_content)
            }
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É
            results['seo_score'] = self._calculate_seo_score(results['analysis'])
            results['recommendations'] = self._generate_recommendations(results['analysis'])
            results['issues'] = self._extract_issues(results['analysis'])
            results['grade'] = self._get_grade(results['seo_score'])
            
            print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {load_time:.3f} —Å–µ–∫")
            return results
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")
            return {
                'file_path': file_path,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def _analyze_title(self, html_content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ title —Ç–µ–≥–∞"""
        
        title_match = re.search(r'<title[^>]*>(.*?)</title>', html_content, re.IGNORECASE | re.DOTALL)
        
        if not title_match:
            return {
                'exists': False,
                'content': '',
                'length': 0,
                'issues': ['–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç title —Ç–µ–≥'],
                'score': 0
            }
        
        title = title_match.group(1).strip()
        title_length = len(title)
        
        issues = []
        score = 100
        
        if title_length < 30:
            issues.append('Title —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (–º–µ–Ω–µ–µ 30 —Å–∏–º–≤–æ–ª–æ–≤)')
            score -= 30
        elif title_length > 60:
            issues.append('Title —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (–±–æ–ª–µ–µ 60 —Å–∏–º–≤–æ–ª–æ–≤)')
            score -= 20
        
        if not title:
            issues.append('–ü—É—Å—Ç–æ–π title —Ç–µ–≥')
            score = 0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        words = title.lower().split()
        unique_words = set(words)
        if len(words) - len(unique_words) > 2:
            issues.append('–ú–Ω–æ–≥–æ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å–ª–æ–≤ –≤ title')
            score -= 15
        
        return {
            'exists': True,
            'content': title,
            'length': title_length,
            'word_count': len(words),
            'unique_words': len(unique_words),
            'issues': issues,
            'score': max(0, score)
        }
    
    def _analyze_meta_description(self, html_content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ meta description"""
        
        meta_desc_pattern = r'<meta[^>]*name=["\']description["\'][^>]*content=["\']([^"\']*)["\']'
        match = re.search(meta_desc_pattern, html_content, re.IGNORECASE)
        
        if not match:
            return {
                'exists': False,
                'content': '',
                'length': 0,
                'issues': ['–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç meta description'],
                'score': 0
            }
        
        description = match.group(1).strip()
        desc_length = len(description)
        
        issues = []
        score = 100
        
        if desc_length < 120:
            issues.append('Meta description —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (–º–µ–Ω–µ–µ 120 —Å–∏–º–≤–æ–ª–æ–≤)')
            score -= 30
        elif desc_length > 160:
            issues.append('Meta description —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (–±–æ–ª–µ–µ 160 —Å–∏–º–≤–æ–ª–æ–≤)')
            score -= 20
        
        if not description:
            issues.append('–ü—É—Å—Ç–æ–π meta description')
            score = 0
        
        return {
            'exists': True,
            'content': description,
            'length': desc_length,
            'issues': issues,
            'score': max(0, score)
        }
    
    def _analyze_meta_viewport(self, html_content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ meta viewport"""
        
        viewport_pattern = r'<meta[^>]*name=["\']viewport["\'][^>]*content=["\']([^"\']*)["\']'
        match = re.search(viewport_pattern, html_content, re.IGNORECASE)
        
        if not match:
            return {
                'exists': False,
                'content': '',
                'issues': ['–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç meta viewport'],
                'score': 0
            }
        
        viewport = match.group(1).strip()
        
        issues = []
        score = 100
        
        if 'width=device-width' not in viewport.lower():
            issues.append('Meta viewport –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç width=device-width')
            score -= 50
        
        if 'initial-scale=1' not in viewport.lower():
            issues.append('Meta viewport –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç initial-scale=1')
            score -= 25
        
        return {
            'exists': True,
            'content': viewport,
            'issues': issues,
            'score': max(0, score)
        }
    
    def _analyze_headings(self, html_content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
        
        headings = {}
        issues = []
        score = 100
        
        for i in range(1, 7):
            pattern = f'<h{i}[^>]*>(.*?)</h{i}>'
            matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
            # –û—á–∏—â–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ—Ç HTML —Ç–µ–≥–æ–≤
            clean_content = []
            for match in matches:
                clean_text = re.sub(r'<[^>]+>', '', match).strip()
                clean_content.append(clean_text)
            
            headings[f'h{i}'] = {
                'count': len(matches),
                'content': clean_content
            }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º H1
        if headings['h1']['count'] == 0:
            issues.append('–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç H1 –∑–∞–≥–æ–ª–æ–≤–æ–∫')
            score -= 40
        elif headings['h1']['count'] > 1:
            issues.append('–ù–µ—Å–∫–æ–ª—å–∫–æ H1 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ')
            score -= 20
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        if headings['h2']['count'] == 0:
            issues.append('–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç H2 –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞')
            score -= 15
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–≥–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        if headings['h3']['count'] > 0 and headings['h2']['count'] == 0:
            issues.append('–ù–∞—Ä—É—à–µ–Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (H3 –±–µ–∑ H2)')
            score -= 10
        
        return {
            'structure': headings,
            'total_headings': sum(h['count'] for h in headings.values()),
            'issues': issues,
            'score': max(0, score)
        }
    
    def _analyze_images(self, html_content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        
        img_tags = re.findall(r'<img[^>]*>', html_content, re.IGNORECASE)
        
        total_images = len(img_tags)
        images_with_alt = 0
        images_with_title = 0
        images_with_lazy_loading = 0
        
        for img in img_tags:
            if 'alt=' in img.lower():
                images_with_alt += 1
            if 'title=' in img.lower():
                images_with_title += 1
            if 'loading=' in img.lower() and 'lazy' in img.lower():
                images_with_lazy_loading += 1
        
        images_without_alt = total_images - images_with_alt
        
        issues = []
        score = 100
        
        if images_without_alt > 0:
            issues.append(f'{images_without_alt} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ alt –∞—Ç—Ä–∏–±—É—Ç–∞')
            score -= min(50, images_without_alt * 10)
        
        if total_images > 3 and images_with_lazy_loading == 0:
            issues.append('–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å lazy loading –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π')
            score -= 10
        
        return {
            'total_images': total_images,
            'images_with_alt': images_with_alt,
            'images_without_alt': images_without_alt,
            'images_with_title': images_with_title,
            'images_with_lazy_loading': images_with_lazy_loading,
            'issues': issues,
            'score': max(0, score)
        }
    
    def _analyze_links(self, html_content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫"""
        
        link_pattern = r'<a[^>]*href=["\']([^"\']*)["\'][^>]*>(.*?)</a>'
        links = re.findall(link_pattern, html_content, re.IGNORECASE | re.DOTALL)
        
        total_links = len(links)
        internal_links = 0
        external_links = 0
        empty_anchor_links = 0
        nofollow_links = 0
        
        for href, anchor_text in links:
            # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Å—Å—ã–ª–∫–∏
            if href.startswith('http'):
                external_links += 1
            elif href.startswith('/') or href.startswith('#') or not href.startswith('http'):
                internal_links += 1
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º anchor text
            clean_anchor = re.sub(r'<[^>]+>', '', anchor_text).strip()
            if not clean_anchor:
                empty_anchor_links += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º nofollow –∞—Ç—Ä–∏–±—É—Ç—ã
        nofollow_pattern = r'<a[^>]*rel=["\'][^"\']*nofollow[^"\']*["\']'
        nofollow_matches = re.findall(nofollow_pattern, html_content, re.IGNORECASE)
        nofollow_links = len(nofollow_matches)
        
        issues = []
        score = 100
        
        if empty_anchor_links > 0:
            issues.append(f'{empty_anchor_links} —Å—Å—ã–ª–æ–∫ —Å –ø—É—Å—Ç—ã–º anchor text')
            score -= min(30, empty_anchor_links * 5)
        
        if external_links > 0 and nofollow_links == 0:
            issues.append('–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å rel="nofollow" –∫ –≤–Ω–µ—à–Ω–∏–º —Å—Å—ã–ª–∫–∞–º')
            score -= 5
        
        return {
            'total_links': total_links,
            'internal_links': internal_links,
            'external_links': external_links,
            'empty_anchor_links': empty_anchor_links,
            'nofollow_links': nofollow_links,
            'issues': issues,
            'score': max(0, score)
        }
    
    def _analyze_structured_data(self, html_content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        # JSON-LD
        json_ld_pattern = r'<script type="application/ld\+json"[^>]*>(.*?)</script>'
        json_ld_matches = re.findall(json_ld_pattern, html_content, re.IGNORECASE | re.DOTALL)
        
        valid_schemas = 0
        schema_types = []
        errors = []
        
        for match in json_ld_matches:
            try:
                data = json.loads(match.strip())
                if '@type' in data:
                    valid_schemas += 1
                    schema_types.append(data['@type'])
            except json.JSONDecodeError:
                errors.append('–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON-LD')
        
        # –ú–∏–∫—Ä–æ–¥–∞–Ω–Ω—ã–µ
        microdata_pattern = r'itemscope[^>]*itemtype="([^"]*)"'
        microdata_matches = re.findall(microdata_pattern, html_content, re.IGNORECASE)
        
        # Open Graph
        og_pattern = r'<meta[^>]*property=["\']og:[^"\']*["\'][^>]*>'
        og_matches = re.findall(og_pattern, html_content, re.IGNORECASE)
        
        issues = []
        score = 50  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        
        if valid_schemas > 0:
            score += min(30, valid_schemas * 15)
        else:
            issues.append('–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç JSON-LD —Ä–∞–∑–º–µ—Ç–∫–∞')
        
        if microdata_matches:
            score += 10
        
        if og_matches:
            score += 10
        else:
            issues.append('–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç Open Graph —Ä–∞–∑–º–µ—Ç–∫–∞')
        
        if errors:
            score -= len(errors) * 10
            issues.extend(errors)
        
        return {
            'json_ld_count': len(json_ld_matches),
            'valid_schemas': valid_schemas,
            'schema_types': schema_types,
            'microdata_count': len(microdata_matches),
            'open_graph_count': len(og_matches),
            'errors': errors,
            'issues': issues,
            'score': max(0, min(100, score))
        }
    
    def _analyze_technical_aspects(self, html_content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∞—Å–ø–µ–∫—Ç–æ–≤"""
        
        issues = []
        score = 100
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º DOCTYPE
        doctype_pattern = r'<!DOCTYPE\s+html>'
        has_doctype = bool(re.search(doctype_pattern, html_content, re.IGNORECASE))
        
        if not has_doctype:
            issues.append('–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç DOCTYPE HTML5')
            score -= 20
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º charset
        charset_pattern = r'<meta[^>]*charset=["\']?utf-8["\']?[^>]*>'
        has_charset = bool(re.search(charset_pattern, html_content, re.IGNORECASE))
        
        if not has_charset:
            issues.append('–ù–µ —É–∫–∞–∑–∞–Ω–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∞ UTF-8')
            score -= 15
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º canonical URL
        canonical_pattern = r'<link[^>]*rel=["\']canonical["\'][^>]*>'
        has_canonical = bool(re.search(canonical_pattern, html_content, re.IGNORECASE))
        
        if not has_canonical:
            issues.append('–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç canonical URL')
            score -= 10
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º robots meta
        robots_pattern = r'<meta[^>]*name=["\']robots["\'][^>]*>'
        has_robots = bool(re.search(robots_pattern, html_content, re.IGNORECASE))
        
        return {
            'has_doctype': has_doctype,
            'has_charset': has_charset,
            'has_canonical': has_canonical,
            'has_robots_meta': has_robots,
            'issues': issues,
            'score': max(0, score)
        }
    
    def _analyze_content_quality(self, html_content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
        text_content = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
        text_content = re.sub(r'<style[^>]*>.*?</style>', '', text_content, flags=re.DOTALL | re.IGNORECASE)
        text_content = re.sub(r'<[^>]+>', ' ', text_content)
        text_content = re.sub(r'\s+', ' ', text_content).strip()
        
        words = text_content.split()
        sentences = text_content.split('.')
        
        word_count = len(words)
        sentence_count = len([s for s in sentences if s.strip()])
        
        # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        word_freq = {}
        stop_words = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–Ω–µ', '—á—Ç–æ', '—ç—Ç–æ', '–∫–∞–∫', '–æ—Ç', '–¥–æ', '–∏–∑', '—É', '–∑–∞', '—Ç–æ', '–∂–µ', '–º—ã', '–≤—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–∏', 'the', 'is', 'at', 'which', 'on', 'and', 'a', 'to', 'as', 'are', 'was', 'will', 'an', 'be', 'or'}
        
        for word in words:
            word_clean = re.sub(r'[^\w]', '', word.lower())
            if len(word_clean) > 3 and word_clean not in stop_words:
                word_freq[word_clean] = word_freq.get(word_clean, 0) + 1
        
        top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        
        issues = []
        score = 100
        
        if word_count < 300:
            issues.append('–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–º–µ–Ω–µ–µ 300 —Å–ª–æ–≤)')
            score -= 40
        elif word_count < 500:
            issues.append('–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å –æ–±—ä–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–æ 500+ —Å–ª–æ–≤')
            score -= 20
        
        if sentence_count > 0:
            avg_sentence_length = word_count / sentence_count
            if avg_sentence_length > 25:
                issues.append('–°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—É—Ö—É–¥—à–∞–µ—Ç —á–∏—Ç–∞–µ–º–æ—Å—Ç—å)')
                score -= 20
        
        readability_score = max(0, min(100, 100 - (avg_sentence_length - 15) * 2)) if sentence_count > 0 else 0
        
        return {
            'word_count': word_count,
            'sentence_count': sentence_count,
            'avg_sentence_length': round(word_count / sentence_count, 1) if sentence_count > 0 else 0,
            'readability_score': round(readability_score, 1),
            'top_keywords': top_keywords,
            'keyword_density': len(set(words)) / len(words) if words else 0,
            'issues': issues,
            'score': max(0, score)
        }
    
    def _analyze_open_graph(self, html_content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ Open Graph —Ç–µ–≥–æ–≤"""
        
        og_tags = {
            'title': r'<meta[^>]*property=["\']og:title["\'][^>]*content=["\']([^"\']*)["\']',
            'description': r'<meta[^>]*property=["\']og:description["\'][^>]*content=["\']([^"\']*)["\']',
            'image': r'<meta[^>]*property=["\']og:image["\'][^>]*content=["\']([^"\']*)["\']',
            'url': r'<meta[^>]*property=["\']og:url["\'][^>]*content=["\']([^"\']*)["\']',
            'type': r'<meta[^>]*property=["\']og:type["\'][^>]*content=["\']([^"\']*)["\']'
        }
        
        found_tags = {}
        issues = []
        score = 100
        
        for tag_name, pattern in og_tags.items():
            match = re.search(pattern, html_content, re.IGNORECASE)
            found_tags[tag_name] = {
                'exists': bool(match),
                'content': match.group(1) if match else ''
            }
            
            if not match:
                issues.append(f'–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç og:{tag_name}')
                score -= 20
        
        return {
            'tags': found_tags,
            'total_found': sum(1 for tag in found_tags.values() if tag['exists']),
            'issues': issues,
            'score': max(0, score)
        }
    
    def _analyze_lang_attribute(self, html_content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∞—Ç—Ä–∏–±—É—Ç–∞ lang"""
        
        lang_pattern = r'<html[^>]*lang=["\']([^"\']*)["\']'
        match = re.search(lang_pattern, html_content, re.IGNORECASE)
        
        if not match:
            return {
                'exists': False,
                'value': '',
                'issues': ['–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∞—Ç—Ä–∏–±—É—Ç lang –≤ —Ç–µ–≥–µ html'],
                'score': 0
            }
        
        lang_value = match.group(1).strip()
        
        issues = []
        score = 100
        
        if len(lang_value) < 2:
            issues.append('–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–∞ lang')
            score -= 50
        
        return {
            'exists': True,
            'value': lang_value,
            'issues': issues,
            'score': max(0, score)
        }
    
    def _calculate_seo_score(self, analysis: Dict[str, Any]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â—É—é SEO –æ—Ü–µ–Ω–∫—É"""
        
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        weights = {
            'title': 0.18,
            'meta_description': 0.15,
            'meta_viewport': 0.08,
            'headings': 0.15,
            'images': 0.08,
            'links': 0.06,
            'structured_data': 0.10,
            'technical': 0.10,
            'content_quality': 0.08,
            'open_graph': 0.02,
            'lang_attribute': 0.02
        }
        
        total_score = 0
        total_weight = 0
        
        for category, weight in weights.items():
            if category in analysis and 'score' in analysis[category]:
                total_score += analysis[category]['score'] * weight
                total_weight += weight
        
        return round(total_score / total_weight if total_weight > 0 else 0, 2)
    
    def _get_grade(self, score: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Ü–µ–Ω–∫—É –±—É–∫–≤–æ–π"""
        if score >= 90:
            return 'A+'
        elif score >= 80:
            return 'A'
        elif score >= 70:
            return 'B'
        elif score >= 60:
            return 'C'
        elif score >= 50:
            return 'D'
        else:
            return 'F'
    
    def _generate_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
        
        recommendations = []
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–∫—Ä–∏—Ç–∏—á–Ω—ã–µ –¥–ª—è SEO)
        if analysis['title']['score'] < 70:
            recommendations.append('üî¥ –ö–†–ò–¢–ò–ß–ù–û: –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ title —Ç–µ–≥ (30-60 —Å–∏–º–≤–æ–ª–æ–≤, —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç)')
        
        if analysis['meta_description']['score'] < 70:
            recommendations.append('üî¥ –ö–†–ò–¢–ò–ß–ù–û: –î–æ–±–∞–≤—å—Ç–µ –∏–ª–∏ —É–ª—É—á—à–∏—Ç–µ meta description (120-160 —Å–∏–º–≤–æ–ª–æ–≤)')
        
        if analysis['headings']['score'] < 70:
            recommendations.append('üî¥ –ö–†–ò–¢–ò–ß–ù–û: –£–ª—É—á—à–∏—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (H1 - –æ–¥–∏–Ω –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É, –¥–æ–±–∞–≤—å—Ç–µ H2-H3)')
        
        # –í–∞–∂–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if analysis['meta_viewport']['score'] < 80:
            recommendations.append('üü° –í–ê–ñ–ù–û: –î–æ–±–∞–≤—å—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π meta viewport –¥–ª—è –º–æ–±–∏–ª—å–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏')
        
        if analysis['images']['score'] < 80:
            recommendations.append('üü° –í–ê–ñ–ù–û: –î–æ–±–∞–≤—å—Ç–µ alt –∞—Ç—Ä–∏–±—É—Ç—ã –∫–æ –≤—Å–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º')
        
        if analysis['structured_data']['score'] < 50:
            recommendations.append('üü° –í–ê–ñ–ù–û: –î–æ–±–∞–≤—å—Ç–µ JSON-LD —Ä–∞–∑–º–µ—Ç–∫—É Schema.org –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞')
        
        if analysis['technical']['score'] < 80:
            recommendations.append('üü° –í–ê–ñ–ù–û: –ò—Å–ø—Ä–∞–≤—å—Ç–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã (DOCTYPE, charset, canonical)')
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
        if analysis['content_quality']['score'] < 70:
            recommendations.append('üü¢ –£–õ–£–ß–®–ï–ù–ò–ï: –£–≤–µ–ª–∏—á—å—Ç–µ –æ–±—ä–µ–º –∏ –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (300+ —Å–ª–æ–≤)')
        
        if analysis['open_graph']['score'] < 60:
            recommendations.append('üü¢ –£–õ–£–ß–®–ï–ù–ò–ï: –î–æ–±–∞–≤—å—Ç–µ Open Graph —Ç–µ–≥–∏ –¥–ª—è —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π')
        
        if analysis['links']['score'] < 80:
            recommendations.append('üü¢ –£–õ–£–ß–®–ï–ù–ò–ï: –£–ª—É—á—à–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ (–¥–æ–±–∞–≤—å—Ç–µ anchor text, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ nofollow)')
        
        return recommendations
    
    def _extract_issues(self, analysis: Dict[str, Any]) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã"""
        
        all_issues = []
        
        for category, data in analysis.items():
            if 'issues' in data:
                all_issues.extend(data['issues'])
        
        return all_issues

def format_results(results: Dict[str, Any]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤—ã–≤–æ–¥–∞"""
    
    if 'error' in results:
        return f"‚ùå –û—à–∏–±–∫–∞: {results['error']}"
    
    output = []
    output.append(f"üìä SEO –ê–Ω–∞–ª–∏–∑: {results['file_path']}")
    output.append(f"üìà SEO –æ—Ü–µ–Ω–∫–∞: {results['seo_score']}/100 (–û—Ü–µ–Ω–∫–∞: {results['grade']})")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    analysis = results['analysis']
    
    output.append(f"\nüìù –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑:")
    output.append(f"  üìå Title: {analysis['title']['score']}/100 ({analysis['title']['length']} —Å–∏–º–≤–æ–ª–æ–≤)")
    if analysis['title']['content']:
        output.append(f"     \"{analysis['title']['content'][:100]}{'...' if len(analysis['title']['content']) > 100 else ''}\"")
    
    output.append(f"  üìÑ Meta Description: {analysis['meta_description']['score']}/100 ({analysis['meta_description']['length']} —Å–∏–º–≤–æ–ª–æ–≤)")
    output.append(f"  üì± Meta Viewport: {analysis['meta_viewport']['score']}/100")
    output.append(f"  üèóÔ∏è  –ó–∞–≥–æ–ª–æ–≤–∫–∏: {analysis['headings']['score']}/100 (H1: {analysis['headings']['structure']['h1']['count']}, H2: {analysis['headings']['structure']['h2']['count']})")
    output.append(f"  üñºÔ∏è  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {analysis['images']['score']}/100 ({analysis['images']['images_without_alt']} –±–µ–∑ alt –∏–∑ {analysis['images']['total_images']})")
    output.append(f"  üîó –°—Å—ã–ª–∫–∏: {analysis['links']['score']}/100 ({analysis['links']['total_links']} –≤—Å–µ–≥–æ, {analysis['links']['external_links']} –≤–Ω–µ—à–Ω–∏—Ö)")
    output.append(f"  üìã –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {analysis['structured_data']['score']}/100 ({analysis['structured_data']['valid_schemas']} –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Ö–µ–º)")
    output.append(f"  ‚öôÔ∏è  –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã: {analysis['technical']['score']}/100")
    output.append(f"  üìù –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {analysis['content_quality']['score']}/100 ({analysis['content_quality']['word_count']} —Å–ª–æ–≤)")
    output.append(f"  üì≤ Open Graph: {analysis['open_graph']['score']}/100 ({analysis['open_graph']['total_found']}/5 —Ç–µ–≥–æ–≤)")
    
    # –¢–æ–ø –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    if analysis['content_quality']['top_keywords']:
        output.append(f"\nüîë –¢–æ–ø –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:")
        for word, count in analysis['content_quality']['top_keywords'][:5]:
            output.append(f"     {word}: {count} —Ä–∞–∑")
    
    # –ü—Ä–æ–±–ª–µ–º—ã
    if results['issues']:
        output.append(f"\n‚ùå –ù–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã ({len(results['issues'])}):")
        for i, issue in enumerate(results['issues'][:8], 1):
            output.append(f"  {i}. {issue}")
        if len(results['issues']) > 8:
            output.append(f"     ... –∏ –µ—â–µ {len(results['issues']) - 8} –ø—Ä–æ–±–ª–µ–º")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if results['recommendations']:
        output.append(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é:")
        for i, rec in enumerate(results['recommendations'][:6], 1):
            output.append(f"  {i}. {rec}")
        if len(results['recommendations']) > 6:
            output.append(f"     ... –∏ –µ—â–µ {len(results['recommendations']) - 6} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
    
    return '\n'.join(output)

def save_results(results: Dict[str, Any], filename: str):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON —Ñ–∞–π–ª"""
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {filename}")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    
    # –ü–æ–∏—Å–∫ HTML —Ñ–∞–π–ª–æ–≤ –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    html_files = list(Path('.').glob('*.html'))
    
    if not html_files:
        print("‚ùå HTML —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
        return
    
    validator = OfflineSEOValidator()
    all_results = []
    
    print("üöÄ –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ SEO —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    print("=" * 80)
    
    for i, file_path in enumerate(html_files, 1):
        print(f"\n[{i}/{len(html_files)}] –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: {file_path}")
        print("-" * 70)
        
        # –ü—Ä–æ–≤–æ–¥–∏–º –∞–Ω–∞–ª–∏–∑
        results = validator.validate_file(str(file_path))
        all_results.append(results)
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print(format_results(results))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if 'error' not in results:
            filename = f"seo_offline_{file_path.stem}.json"
            save_results(results, filename)
    
    # –°–≤–æ–¥–∫–∞ –ø–æ –≤—Å–µ–º —Ç–µ—Å—Ç–∞–º
    print(f"\n{'='*80}")
    print("üìã –°–í–û–î–ö–ê –ü–û –í–°–ï–ú –¢–ï–°–¢–ê–ú")
    print("=" * 80)
    
    successful_tests = [r for r in all_results if 'error' not in r]
    failed_tests = [r for r in all_results if 'error' in r]
    
    print(f"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {len(successful_tests)}")
    print(f"‚ùå –ù–µ—É–¥–∞—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {len(failed_tests)}")
    
    if successful_tests:
        scores = [r['seo_score'] for r in successful_tests]
        avg_score = sum(scores) / len(scores)
        best_test = max(successful_tests, key=lambda x: x['seo_score'])
        worst_test = min(successful_tests, key=lambda x: x['seo_score'])
        
        print(f"üìà –°—Ä–µ–¥–Ω—è—è SEO –æ—Ü–µ–Ω–∫–∞: {avg_score:.1f}/100")
        print(f"üèÜ –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {Path(best_test['file_path']).name} ({best_test['seo_score']}/100, {best_test['grade']})")
        print(f"üî¥ –¢—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è: {Path(worst_test['file_path']).name} ({worst_test['seo_score']}/100, {worst_test['grade']})")
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫
        grades = [r['grade'] for r in successful_tests]
        grade_count = {}
        for grade in grades:
            grade_count[grade] = grade_count.get(grade, 0) + 1
        
        print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫:")
        for grade in ['A+', 'A', 'B', 'C', 'D', 'F']:
            if grade in grade_count:
                print(f"  {grade}: {grade_count[grade]} —Å—Ç—Ä–∞–Ω–∏—Ü")
        
        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        all_issues = []
        for result in successful_tests:
            all_issues.extend(result['issues'])
        
        if all_issues:
            from collections import Counter
            common_issues = Counter(all_issues).most_common(5)
            
            print(f"\nüéØ –ù–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:")
            for issue, count in common_issues:
                print(f"  ‚Ä¢ {issue} (–≤ {count} —Ñ–∞–π–ª–∞—Ö)")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â–∏–π –æ—Ç—á–µ—Ç
    timestamp = datetime.now()
    full_report = {
        'timestamp': timestamp.isoformat(),
        'summary': {
            'total_tests': len(all_results),
            'successful': len(successful_tests),
            'failed': len(failed_tests),
            'average_score': avg_score if successful_tests else 0,
            'grade_distribution': grade_count if successful_tests else {}
        },
        'results': all_results
    }
    
    save_results(full_report, f'seo_offline_full_report_{timestamp.strftime("%Y%m%d_%H%M%S")}.json')
    
    print(f"\nüéâ –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ SEO —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìÅ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ JSON —Ñ–∞–π–ª–∞—Ö")

if __name__ == "__main__":
    main()
